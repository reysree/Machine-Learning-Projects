{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 584 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>+1</th>\n",
       "      <th>This book is such a life saver.  It has been so helpful to be able to go back to track trends, answer pediatrician questions, or communicate with each other when you are up at different times of the night with a newborn.  I think it is one of those things that everyone should be required to have before they leave the hospital.  We went through all the pages of the newborn version, then moved to the infant version, and will finish up the second infant book (third total) right as our baby turns 1.  See other things that are must haves for baby at [...]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I bought this a few times for my older son and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This is great for basics, but I wish the space...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   +1  \\\n",
       "0   1   \n",
       "1   1   \n",
       "\n",
       "  This book is such a life saver.  It has been so helpful to be able to go back to track trends, answer pediatrician questions, or communicate with each other when you are up at different times of the night with a newborn.  I think it is one of those things that everyone should be required to have before they leave the hospital.  We went through all the pages of the newborn version, then moved to the infant version, and will finish up the second infant book (third total) right as our baby turns 1.  See other things that are must haves for baby at [...]  \n",
       "0  I bought this a few times for my older son and...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1  This is great for basics, but I wish the space...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the train dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "with open('1675140108_9696712_1567602457_1187546_train_file.dat', 'r') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    header = next(reader)\n",
    "    rows = [row for row in reader]\n",
    "    \n",
    "with open('train_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "train = pd.read_csv('train_data.csv')\n",
    "#train.columns = ['output','text']\n",
    "#train['clean_text'] = train.loc[:, 'text']\n",
    "#print(\"Length of Train File: \"+str(len(train)))\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test File: 18506\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Perfect for new parents. We were able to keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Helps me know exactly how my babies day has go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text\n",
       "0  Perfect for new parents. We were able to keep ...\n",
       "1  Helps me know exactly how my babies day has go..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the test dataset\n",
    "\n",
    "test_file = '1675140109_010778_1567602457_126649_test.dat'\n",
    "\n",
    "testList = []\n",
    "with open(test_file, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    rows = content.splitlines()\n",
    "    for row in rows:\n",
    "        testList.append(row)\n",
    "\n",
    "test = pd.DataFrame({'final_text': testList})\n",
    "print(\"Length of test File: \"+str(len(test)))\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# step 1 is to remove all punctuation marks from the train data and test data\n",
    "\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "#train data\n",
    "\n",
    "for column in train.columns[2:]:\n",
    "    for row in range(len(train)):\n",
    "        collect = []\n",
    "        for word in str(train['clean_text'][row]):\n",
    "            if word not in string.punctuation:\n",
    "                collect.append(word)\n",
    "            elif word in string.punctuation:\n",
    "                collect.append(\" \")\n",
    "        sample = \"\".join(collect)\n",
    "        sample = sample.replace(\"  \",\" \")\n",
    "        train['clean_text'][row] = sample\n",
    "        \n",
    "#test data\n",
    "\n",
    "for column in range(1):\n",
    "    for row in range(1):\n",
    "        collect = []\n",
    "        for word in str(test['final_text'][row]):\n",
    "            if word not in string.punctuation:\n",
    "                collect.append(word)\n",
    "            elif word in string.punctuation:\n",
    "                collect.append(\" \")\n",
    "        sample = \"\".join(collect)\n",
    "        sample = sample.replace(\"  \",\" \")\n",
    "        test['final_text'][row] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#step 2 make the whole text data into lower case for train and test datasets\n",
    "\n",
    "#train\n",
    "\n",
    "for column in train.columns[2:]:\n",
    "    for row in range(len(train)):\n",
    "        sample = (str(train['clean_text'][row])).lower()\n",
    "        train['clean_text'][row] = sample\n",
    "        \n",
    "#test\n",
    "\n",
    "for column in test.columns[1:]:\n",
    "    for row in range(len(test)):\n",
    "        sample = (str(test['final_text'][row])).lower()\n",
    "        test['final_text'][row] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#step 3 is to tokenize the text data for both train and test datasets\n",
    "\n",
    "import re\n",
    "\n",
    "#train\n",
    "for column in train.columns[2:]:\n",
    "    for row in range(len(train)):\n",
    "        sample = str(train['clean_text'][row])\n",
    "        tokens = re.split('W+',sample)\n",
    "        train['clean_text'][row] = tokens\n",
    "\n",
    "#test\n",
    "for column in test.columns[1:]:\n",
    "    for row in range(len(test)):\n",
    "        sample = str(test['final_text'][row])\n",
    "        tokens = re.split('W+',sample)\n",
    "        test['final_text'][row] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#step 4 is to remove stop words from the text data\n",
    "\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#train\n",
    "for column in train.columns[2:]:\n",
    "    for row in range(len(train)):\n",
    "        train['clean_text'][row] = \" \".join([i for i in str(train['clean_text'][row]).split(\" \") if i not in stopwords])\n",
    "        \n",
    "#test\n",
    "for column in test.columns[1:]:\n",
    "    for row in range(len(test)):\n",
    "        test['final_text'][row] = \" \".join([i for i in str(test['final_text'][row]).split(\" \") if i not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#step 5 is to use lemmatization on the tokenized text\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#train\n",
    "for column in train.columns[1:]:\n",
    "    for row in range(1):\n",
    "        collect = []\n",
    "        words = str(train['clean_text'][row]).split(\" \")\n",
    "        for word in words:\n",
    "            collect.append(lemmatizer.lemmatize(word))\n",
    "        lemm_text = \" \".join(collect)\n",
    "        train['clean_text'][row] = lemm_text\n",
    "        \n",
    "#test\n",
    "for column in test.columns[1:]:\n",
    "    for row in range(1):\n",
    "        collect = []\n",
    "        words = str(test['final_text'][row]).split(\" \")\n",
    "        for word in words:\n",
    "            collect.append(lemmatizer.lemmatize(word))\n",
    "        lemm_text = \" \".join(collect)\n",
    "        test['final_text'][row] = lemm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using n-grams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "ngrams = vectorizer.fit_transform(train[\"clean_text\"])\n",
    "ngrams_test = vectorizer.transform(test[\"final_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ngrams_test matrix has 18506 rows and 490229 columns.\n",
      "The ngrams_train matrix has 18506 rows and 490229 columns.\n"
     ]
    }
   ],
   "source": [
    "#checking if the number of rows in train and test dataset are same\n",
    "\n",
    "rows, columns = ngrams_test.shape\n",
    "print(\"The ngrams_test matrix has\", rows, \"rows and\", columns, \"columns.\")\n",
    "\n",
    "rows, columns = ngrams.shape\n",
    "print(\"The ngrams_train matrix has\", rows, \"rows and\", columns, \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model using logistic regression\n",
    "\n",
    "X = ngrams\n",
    "y = train['output']\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "#predicting the output using the whole test data on the model trained\n",
    "prediction= clf.predict(ngrams_test)\n",
    "\n",
    "#saving the model in csv format\n",
    "result_df = pd.DataFrame(prediction, columns = ['output'])\n",
    "result_df.to_csv('main_preds_ngrams_lemm1.dat', sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
